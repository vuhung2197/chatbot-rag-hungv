import pool from '../../../db.js';
import { getEmbedding } from '../../../services/embeddingVector.js';
import { retrieveTopChunks } from '../../../services/rag_retrieve.js';
import { hashQuestion } from '../../../utils/hash.js';
import { StatusCodes } from 'http-status-codes';
import '../../../bootstrap/env.js';
import { trackUsage } from '../usage/usage.controller.js';
import { getOrCreateConversationId } from './conversation.controller.js';
import {
    multiStageRetrieval,
    semanticClustering,
    multiHopReasoning,
    fuseContext,
    adaptiveRetrieval,
    rerankContext,
    rerankWithCohere
} from '../../../services/advancedRAGFixed.js';
import { callLLM } from '../../../services/llmService.js';
import { performWebSearch } from '../../../services/webSearch.service.js';
import { classifyIntent, INTENTS } from '../../../services/intentRouter.js';

// ==================== HELPER FUNCTIONS ====================

/**
 * Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n AI tr·∫£ l·ªùi th√†nh Markdown gi·ªëng ChatGPT.
 */
function toMarkdown(text) {
    if (!text) return '';

    const paragraphs = text.split(/\n{2,}/);
    const firstPara = paragraphs.shift()?.trim();
    let markdown = '';

    // B1: C√¢u ƒë·∫ßu ti√™n in ƒë·∫≠m
    if (firstPara) {
        const sentences = firstPara.split(/(?<=\.)\s+/);
        const firstSentence = sentences.shift();
        markdown += `**${firstSentence.trim()}**\n\n`;
        if (sentences.length) {
            markdown += `${sentences.join(' ')}\n\n`;
        }
    }

    // B2: Duy·ªát c√°c ƒëo·∫°n c√≤n l·∫°i
    for (let para of paragraphs) {
        para = para.trim();
        if (!para) continue;

        const isList =
            para.startsWith('- ') ||
            para.startsWith('* ') ||
            /^[‚Ä¢\-+]\s/.test(para) ||
            (/(,|\.)\s/.test(para) && para.length < 200);

        if (isList) {
            const points = para
                .split(/(?:^|\n)[‚Ä¢\-+*]?\s*/)
                .map((p) => p.trim())
                .filter((p) => p.length > 0);
            points.forEach((point) => {
                markdown += `- ${point}\n`;
            });
            markdown += '\n';
        } else {
            markdown += `${para}\n\n`;
        }
    }

    return markdown.trim();
}

/**
 * Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n AI tr·∫£ l·ªùi th√†nh Markdown v·ªõi c·∫•u tr√∫c t·ªët h∆°n (Advanced)
 */
function toAdvancedMarkdown(text) {
    if (!text) return '';

    const paragraphs = text.split(/\n{2,}/);
    let markdown = '';

    for (const para of paragraphs) {
        const trimmed = para.trim();
        if (!trimmed) continue;

        // Detect headers
        if (trimmed.match(/^#{1,6}\s/)) {
            markdown += `${trimmed}\n\n`;
            continue;
        }

        // Detect lists
        if (trimmed.startsWith('- ') || trimmed.startsWith('* ') || /^[‚Ä¢\-+]\s/.test(trimmed)) {
            const points = trimmed
                .split(/(?:^|\n)[‚Ä¢\-+*]?\s*/)
                .map(p => p.trim())
                .filter(p => p.length > 0);

            points.forEach(point => {
                markdown += `- ${point}\n`;
            });
            markdown += '\n';
            continue;
        }

        // Detect code blocks
        if (trimmed.startsWith('```')) {
            markdown += `${trimmed}\n\n`;
            continue;
        }

        // Regular paragraph
        markdown += `${trimmed}\n\n`;
    }

    return markdown.trim();
}

/**
 * ·∫®n th√¥ng tin nh·∫°y c·∫£m
 */
export function maskSensitiveInfo(text, mapping = {}) {
    let counter = 1;
    // S·ªë ƒëi·ªán tho·∫°i
    text = text.replace(/\b\d{2,4}[-\s]?\d{3,4}[-\s]?\d{3,4}\b/g, (match) => {
        const key = `[PHONE_${counter++}]`;
        mapping[key] = match;
        return key;
    });
    // Email
    text = text.replace(
        /\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b/gi,
        (match) => {
            const key = `[EMAIL_${counter++}]`;
            mapping[key] = match;
            return key;
        }
    );
    // ƒê·ªãa ch·ªâ
    text = text.replace(
        /(\d{1,4}\s?[\w\s,./-]+(ƒë∆∞·ªùng|ph·ªë|t√≤a nh√†)[^\n,.]*)/gi,
        (match) => {
            const key = `[ADDR_${counter++}]`;
            mapping[key] = match;
            return key;
        }
    );
    return text;
}

/**
 * Kh√¥i ph·ª•c th√¥ng tin nh·∫°y c·∫£m
 */
export function unmaskSensitiveInfo(text, mapping) {
    for (const [key, value] of Object.entries(mapping)) {
        text = text.replaceAll(key, value);
    }
    return text;
}

// function callLLM moved to services/llmService.js

/**
 * Log unanswered questions
 */
async function logUnanswered(question) {
    try {
        const hash = hashQuestion(question);
        const [rows] = await pool.execute(
            'SELECT 1 FROM unanswered_questions WHERE hash = ? LIMIT 1',
            [hash]
        );
        if (rows.length === 0) {
            await pool.execute(
                'INSERT INTO unanswered_questions (question, hash, created_at) VALUES (?, ?, NOW())',
                [question, hash]
            );
        }
    } catch (e) {
        console.warn('‚ö†Ô∏è Kh√¥ng th·ªÉ ghi log unanswered:', e.message);
    }
}

/**
 * G·ªçi OpenAI ChatGPT (Basic)
 */
export async function askChatGPT(
    question,
    context,
    systemPrompt = 'B·∫°n l√† tr·ª£ l√Ω AI chuy√™n tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.',
    model
) {
    const mapping = {};
    const maskedQuestion = maskSensitiveInfo(question, mapping);

    let prompt = '';
    if (context && context.trim().length > 0) {
        const maskedContext = maskSensitiveInfo(context, mapping);
        prompt = `Th√¥ng tin tham kh·∫£o:\n${maskedContext}\n\nC√¢u h·ªèi: ${maskedQuestion}`;
    } else {
        prompt = maskedQuestion;
    }

    const messages = [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: prompt },
    ];

    let reply = await callLLM(model, messages, 0.2, 512);
    reply = unmaskSensitiveInfo(reply, mapping);

    return reply;
}

/**
 * G·ªçi LLM v·ªõi context n√¢ng cao (Advanced)
 */
async function askAdvancedChatGPT(question, context, systemPrompt, model) {
    const maxContextLength = 6000;
    const truncatedContext = context.length > maxContextLength
        ? `${context.substring(0, maxContextLength)}...`
        : context;

    console.log(`üìù Context size: ${context.length} chars, truncated to: ${truncatedContext.length} chars`);

    const prompt = `# C√¢u h·ªèi: ${question}
 
 # Th√¥ng tin tham kh·∫£o:
 ${truncatedContext}
 
 # H∆∞·ªõng d·∫´n:
 H√£y ph√¢n t√≠ch c√¢u h·ªèi v√† s·ª≠ d·ª•ng th√¥ng tin tham kh·∫£o ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi to√†n di·ªán. 
 K·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn m·ªôt c√°ch logic v√† c√≥ c·∫•u tr√∫c.`;

    const messages = [
        { role: 'system', content: (systemPrompt || '').substring(0, 4000) },
        { role: 'user', content: prompt.substring(0, 8000) }
    ];

    const reply = await callLLM(model, messages, 0.3, 800);
    return reply;
}


// ==================== NEW HELPER FUNCTIONS (CONTEXT) ====================

/**
 * L·∫•y l·ªãch s·ª≠ chat g·∫ßn nh·∫•t ƒë·ªÉ l√†m context
 */
async function getChatHistory(userId, conversationId, limit = 6) {
    if (!conversationId || !userId) {
        console.log('‚ö†Ô∏è getChatHistory: Missing userId or conversationId', { userId, conversationId });
        return [];
    }
    try {
        console.log(`üîç Fetching history for User: ${userId}, Conv: ${conversationId}`);
        const [rows] = await pool.execute(
            `SELECT question, bot_reply FROM user_questions 
             WHERE user_id = ? AND conversation_id = ? 
             ORDER BY created_at DESC LIMIT ?`,
            [userId, conversationId, limit]
        );
        console.log(`‚úÖ Found ${rows.length} history items.`);

        // Rows are DESC (newest first), so reverse them to get chronological order
        const history = [];
        for (let i = rows.length - 1; i >= 0; i--) {
            const row = rows[i];
            if (row.question) history.push({ role: 'user', content: row.question });
            if (row.bot_reply) history.push({ role: 'assistant', content: row.bot_reply });
        }
        return history;
    } catch (e) {
        console.warn('‚ö†Ô∏è Filed to fetch history:', e.message);
        return [];
    }
}

/**
 * Vi·∫øt l·∫°i c√¢u h·ªèi d·ª±a tr√™n l·ªãch s·ª≠ ƒë·ªÉ search t·ªët h∆°n (Query Expansion)
 */
async function rewriteQuery(message, history, modelConfig) {
    if (!history || history.length === 0) return message;

    // Create a mini-history string (last 2 turns)
    const historyText = history.slice(-4).map(h => `${h.role === 'user' ? 'User' : 'AI'}: ${h.content}`).join('\n');

    const systemPrompt = `B·∫°n l√† chuy√™n gia v·ªÅ ng√¥n ng·ªØ. Nhi·ªám v·ª• c·ªßa b·∫°n l√† vi·∫øt l·∫°i c√¢u h·ªèi follow-up c·ªßa ng∆∞·ªùi d√πng th√†nh m·ªôt c√¢u h·ªèi ƒë·ªôc l·∫≠p (Standalone Question) ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh, d·ª±a tr√™n l·ªãch s·ª≠ h·ªôi tho·∫°i.
- GI·ªÆ NGUY√äN n·ªôi dung c·ªët l√µi c·ªßa c√¢u h·ªèi.
- THAY TH·∫æ c√°c ƒë·∫°i t·ª´ thay th·∫ø (n√≥, anh ·∫•y, c√°i ƒë√≥...) b·∫±ng danh t·ª´ c·ª• th·ªÉ t·ª´ l·ªãch s·ª≠.
- N·∫æU c√¢u h·ªèi ƒë√£ r√µ r√†ng, gi·ªØ nguy√™n.
- CH·ªà TR·∫¢ V·ªÄ C√ÇU H·ªéI ƒê√É VI·∫æT L·∫†I. KH√îNG tr·∫£ l·ªùi c√¢u h·ªèi.`;

    try {
        const rewritten = await callLLM(modelConfig, [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: `L·ªãch s·ª≠ h·ªôi tho·∫°i:\n${historyText}\n\nC√¢u h·ªèi hi·ªán t·∫°i: ${message}\n\nVi·∫øt l·∫°i:` }
        ], 0.3, 200);

        const clean = rewritten.trim().replace(/^['"]|['"]$/g, '');
        console.log(`üìù Query Rewrite: "${message}" -> "${clean}"`);
        return clean;
    } catch (e) {
        console.error('Rewrite query failed:', e.message);
        return message;
    }
}

// ==================== CONTROLLER FUNCTIONS ====================

/**
 * X·ª≠ l√Ω API chat ch√≠nh s·ª≠ d·ª•ng thu·∫ßn RAG.
 */
/**
 * CONTROLLER CH√çNH: X·ª≠ l√Ω Chat v·ªõi Advanced RAG Pipeline
 * Quy tr√¨nh: Router -> Hybrid Retrieval -> Re-ranking -> Context Fusion -> LLM
 */
export async function chat(req, res) {
    const { message, model, conversationId } = req.body;
    const userId = req.user?.id;

    if (!message)
        return res.status(StatusCodes.BAD_REQUEST).json({ reply: 'No message!' });

    // Validate Model Config
    const modelConfig = (model && model.url && model.name)
        ? model
        : { url: 'https://api.openai.com/v1', name: 'gpt-4o-mini' }; // Default fallback

    try {
        // Prepare History & Context
        let history = [];
        let processingMessage = message; // Message used for Processing (Intent, Search) - NOT display

        if (userId && conversationId) {
            history = await getChatHistory(userId, conversationId);
            if (history.length > 0) {
                processingMessage = await rewriteQuery(message, history, modelConfig);
            }
        }

        // =================================================================
        // B∆Ø·ªöC 1: ROUTER - Ph√¢n lo·∫°i √Ω ƒë·ªãnh (Intent Classification)
        // =================================================================
        const { intent, reasoning } = await classifyIntent(processingMessage, modelConfig);
        console.log(`üß≠ Intent: ${intent} | ${reasoning}`);

        // X·ª≠ l√Ω c√°c intent kh√¥ng c·∫ßn tra c·ª©u ki·∫øn th·ª©c (Non-Knowledge)
        if (intent === INTENTS.OFF_TOPIC) {
            return res.json({
                reply: "Xin l·ªói, t√¥i kh√¥ng th·ªÉ th·∫£o lu·∫≠n v·ªÅ ch·ªß ƒë·ªÅ n√†y do c√°c quy ƒë·ªãnh v·ªÅ an to√†n n·ªôi dung.",
                reasoning_steps: [`Intent: OFF_TOPIC (${reasoning})`, 'Action: Refusal'],
                chunks_used: []
            });
        }

        if (intent === INTENTS.GREETING) {
            const directSystemPrompt = "B·∫°n l√† tr·ª£ l√Ω AI th√¢n thi·ªán. H√£y tr·∫£ l·ªùi ng∆∞·ªùi d√πng m·ªôt c√°ch t·ª± nhi√™n, l·ªãch s·ª± v√† ng·∫Øn g·ªçn.";
            const messages = [
                { role: 'system', content: directSystemPrompt },
                ...history.slice(-4),
                { role: 'user', content: message }
            ];
            const directReply = await callLLM(modelConfig, messages, 0.7, 200);
            return res.json({
                reply: directReply,
                reasoning_steps: [`Intent: GREETING (${reasoning})`, 'Action: Direct Chat (No RAG)'],
                chunks_used: []
            });
        }

        // X·ª≠ l√Ω t√¨m ki·∫øm web (Live Search)
        if (intent === INTENTS.LIVE_SEARCH) {
            console.log('üåç Performing LIVE_SEARCH...');
            const t0 = Date.now();
            const searchContext = await performWebSearch(processingMessage);

            const systemPrompt = `B·∫°n l√† m·ªôt tr·ª£ l√Ω c·∫≠p nh·∫≠t tin t·ª©c th√¥ng minh. 
Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n k·∫øt qu·∫£ t√¨m ki·∫øm web m·ªõi nh·∫•t ƒë∆∞·ª£c cung c·∫•p.
Th·ªùi gian hi·ªán t·∫°i: ${new Date().toLocaleString('vi-VN')}

Y√™u c·∫ßu:
1. Tr·∫£ l·ªùi ch√≠nh x√°c, ng·∫Øn g·ªçn v√† ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ.
2. N·∫æU k·∫øt qu·∫£ t√¨m ki·∫øm c√≥ ch·ª©a th√¥ng tin, H√ÉY D·∫™N NGU·ªíN (Link URL) ·ªü cu·ªëi c√¢u tr·∫£ l·ªùi d·∫°ng [Title](URL).
3. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y th√†nh th·∫≠t n√≥i kh√¥ng bi·∫øt.
4. Tr√¨nh b√†y ƒë·∫πp b·∫±ng Markdown.`;

            const replyRaw = await callLLM(modelConfig, [
                { role: 'system', content: systemPrompt },
                ...history.slice(-4),
                { role: 'user', content: `# C√¢u h·ªèi: ${message}\n\n${searchContext}` }
            ], 0.4, 800);

            const reply = toAdvancedMarkdown(replyRaw);
            const processTime = Date.now() - t0;

            const reasoningSteps = [
                `Intent: LIVE_SEARCH (${reasoning})`,
                `Performed Web Search via Tavily AI`,
                `Synthesized answer from top web results`,
                `Processing time: ${processTime}ms`
            ];

            // Save to DB and return response (similar logic)
            if (userId) {
                const finalConversationId = await getOrCreateConversationId(userId, conversationId);
                const metadata = { processing_time: processTime, model: modelConfig.name, intent: intent, source: 'web_search' };
                await pool.execute(
                    'INSERT INTO user_questions (user_id, conversation_id, conversation_title, question, bot_reply, is_answered, metadata) VALUES (?, ?, ?, ?, ?, ?, ?)',
                    [userId, finalConversationId, null, message, reply, true, JSON.stringify(metadata)]
                );
                await trackUsage(userId, 'web_search', { tokens: searchContext.length });
                return res.json({
                    reply,
                    conversationId: finalConversationId,
                    chunks_used: [], // Web search doesn't use RAG chunks
                    reasoning_steps: reasoningSteps
                });
            }

            return res.json({
                reply,
                chunks_used: [],
                reasoning_steps: reasoningSteps
            });
        }

        // =================================================================
        // B∆Ø·ªöC 2: RETRIEVAL - T√¨m ki·∫øm d·ªØ li·ªáu (Hybrid Search)
        // =================================================================
        console.log('üß† Starting RAG Pipeline for:', message);
        const t0 = Date.now();

        // 2.1 T·∫°o Embedding cho c√¢u h·ªèi (D√πng rewritten query)
        let questionEmbedding;
        try {
            questionEmbedding = await getEmbedding(processingMessage);
        } catch (error) {
            console.error('‚ùå Embedding Error:', error);
            return res.json({ reply: 'L·ªói h·ªá th·ªëng khi x·ª≠ l√Ω c√¢u h·ªèi (Embedding).' });
        }

        // 2.2 Adaptive Retrieval Parameters (Tu·ª≥ ch·ªçn: c√≥ th·ªÉ hardcode n·∫øu mu·ªën ƒë∆°n gi·∫£n)
        const retrievalParams = await adaptiveRetrieval(processingMessage, questionEmbedding);

        // 2.3 Th·ª±c hi·ªán t√¨m ki·∫øm (Vector + Keyword + RRF Fusion)
        const rawChunks = await multiStageRetrieval(
            questionEmbedding,
            processingMessage,
            retrievalParams.maxChunks
        );

        // =================================================================
        // B∆Ø·ªöC 3: RE-RANKING & THRESHOLDING (Cohere)
        // =================================================================
        let finalChunks = [];
        try {
            finalChunks = await rerankContext(rawChunks, questionEmbedding, processingMessage);
        } catch (error) {
            console.error('‚ùå Re-ranking Error:', error);
            finalChunks = rawChunks; // Fallback
        }

        if (finalChunks.length === 0) {
            await logUnanswered(message);
            return res.json({
                reply: 'T√¥i ch∆∞a c√≥ ƒë·ªß th√¥ng tin trong c∆° s·ªü d·ªØ li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y ch√≠nh x√°c.',
                reasoning_steps: ['Retrieval returned 0 relevant chunks (after thresholding)'],
                chunks_used: []
            });
        }

        // =================================================================
        // B∆Ø·ªöC 4: CONTEXT SYNTHESIS (T·ªïng h·ª£p ng·ªØ c·∫£nh)
        // =================================================================

        // 4.1 Ti·ªÅn x·ª≠ l√Ω: Semantic Clustering & Reasoning (Advanced)
        let clusters = [], reasoningChains = [];
        if (retrievalParams.useMultiHop) {
            // Ch·ªâ ch·∫°y n·∫øu c·∫ßn thi·∫øt ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian
            try {
                const results = await Promise.all([
                    semanticClustering(finalChunks, questionEmbedding),
                    multiHopReasoning(finalChunks.slice(0, 5), questionEmbedding, processingMessage)
                ]);
                clusters = results[0];
                reasoningChains = results[1];
            } catch (e) { console.warn('Advanced synthesis skipped:', e); }
        }

        // 4.2 T·∫°o prompt ng·ªØ c·∫£nh
        const fusedContext = fuseContext(finalChunks, reasoningChains, processingMessage);

        // =================================================================
        // B∆Ø·ªöC 5: LLM GENERATION (Sinh c√¢u tr·∫£ l·ªùi)
        // =================================================================
        const systemPrompt = `B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y.
N·∫øu th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh, h√£y n√≥i "T√¥i kh√¥ng bi·∫øt".
Lu√¥n tr√≠ch d·∫´n ngu·ªìn (n·∫øu c√≥ th·ªÉ) v√† tr√¨nh b√†y m·∫°ch l·∫°c b·∫±ng Markdown.

---
${fusedContext}
---`;

        let reply = '';
        try {
            // Cho ph√©p context d√†i h∆°n cho c√¢u h·ªèi ph·ª©c t·∫°p
            const replyRaw = await callLLM(modelConfig, [
                { role: 'system', content: systemPrompt },
                ...history.slice(-6), // Pass history (max 6 turns)
                { role: 'user', content: message }
            ], 0.3, 1000);

            // Format l·∫°i markdown n·∫øu c·∫ßn (tu·ª≥ ch·ªçn)
            reply = toAdvancedMarkdown(replyRaw);

        } catch (error) {
            console.error('‚ùå LLM Generation Error:', error);
            reply = "Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi.";
        }

        const t1 = Date.now();
        const processTime = t1 - t0;
        console.log(`‚è±Ô∏è Total RAG Time: ${processTime}ms`);

        // =================================================================
        // B∆Ø·ªöC 6: LOGGING & RESPONSE
        // =================================================================
        const reasoningSteps = [
            `Intent: ${intent}`,
            `Retrieved ${rawChunks.length} chunks (Hybrid Search)`,
            `Selected ${finalChunks.length} chunks after Re-ranking`,
            `Processing time: ${processTime}ms`
        ];

        // Format chunks for client
        const chunksForClient = finalChunks.map(c => ({
            id: c.id,
            title: c.title,
            content: c.content,
            score: c.final_score || c.score,
            source: c.source_type || 'unknown'
        }));

        if (userId) {
            // L∆∞u v√†o DB n·∫øu ƒë√£ ƒëƒÉng nh·∫≠p
            const finalConversationId = await getOrCreateConversationId(userId, conversationId);

            // Logic t·∫°o title h·ªôi tho·∫°i m·ªõi (n·∫øu c·∫ßn) - gi·ªØ nguy√™n logic c≈©
            const [existingMessages] = await pool.execute(
                'SELECT COUNT(*) as count FROM user_questions WHERE user_id = ? AND conversation_id = ?',
                [userId, finalConversationId]
            );
            let conversationTitle = null;
            if (existingMessages[0].count === 0) {
                conversationTitle = message.trim().substring(0, 50);
            }

            const metadata = {
                processing_time: processTime,
                model: modelConfig.name,
                total_chunks: finalChunks.length,
                intent: intent
            };

            await pool.execute(
                'INSERT INTO user_questions (user_id, conversation_id, conversation_title, question, bot_reply, is_answered, metadata) VALUES (?, ?, ?, ?, ?, ?, ?)',
                [userId, finalConversationId, conversationTitle, message, reply, true, JSON.stringify(metadata)]
            );

            await trackUsage(userId, 'advanced_rag', { tokens: fusedContext.length });

            res.json({
                reply,
                conversationId: finalConversationId,
                chunks_used: chunksForClient,
                reasoning_steps: reasoningSteps
            });
        } else {
            // Guest mode
            res.json({
                reply,
                chunks_used: chunksForClient,
                reasoning_steps: reasoningSteps
            });
        }

    } catch (err) {
        console.error('‚ùå Critical Error in Chat Controller:', err);
        res.status(500).json({ reply: 'ƒê√£ x·∫£y ra l·ªói nghi√™m tr·ªçng ph√≠a m√°y ch·ªß.' });
    }
}

/**
 * API l·∫•y l·ªãch s·ª≠ chat g·∫ßn nh·∫•t (gi·ªØ nguy√™n legacy endpoint).
 */
export async function history(req, res) {
    const userId = req.user?.id;

    if (!userId)
        return res
            .status(StatusCodes.UNAUTHORIZED)
            .json({ error: 'Ch∆∞a ƒëƒÉng nh·∫≠p' });

    try {
        const [rows] = await pool.execute(
            `SELECT id, question, bot_reply, is_answered, created_at 
       FROM user_questions 
       WHERE user_id = ? 
       ORDER BY created_at DESC 
       LIMIT 50`,
            [userId]
        );
        res.json(rows);
    } catch (err) {
        console.error('‚ùå L·ªói khi l·∫•y l·ªãch s·ª≠ c√¢u h·ªèi:', err);
        res.status(500).json({ error: 'L·ªói server' });
    }
}

/**
 * Advanced Chat API v·ªõi Multi-Chunk Reasoning
 */
// function advancedChat is now deprecated as main chat function has been upgraded.
export const advancedChat = chat;

/**
 * Get advanced RAG statistics
 */
export async function getAdvancedRAGStats(req, res) {
    try {
        const [stats] = await pool.execute(`
      SELECT 
        COUNT(*) as total_questions,
        AVG(CAST(metadata->>'total_chunks' AS NUMERIC)) as avg_chunks,
        AVG(CAST(metadata->>'processing_time' AS NUMERIC)) as avg_processing_time,
        COUNT(CASE WHEN CAST(metadata->>'reasoning_chains' AS NUMERIC) > 0 THEN 1 END) as complex_questions
      FROM user_questions 
      WHERE metadata IS NOT NULL
    `);

        res.json({
            success: true,
            stats: stats[0]
        });
    } catch (err) {
        console.error('‚ùå L·ªói get stats:', err);
        res.status(500).json({ success: false, error: err.message });
    }
}

/**
 * Controller x·ª≠ l√Ω Chat v·ªõi c∆° ch·∫ø Streaming (Server-Sent Events)
 * Endpoint: /chat/stream
 */
export async function streamChat(req, res) {
    const { message, model, conversationId } = req.body;
    const userId = req.user?.id;

    if (!message) return res.status(400).json({ error: 'No message provided' });

    // Validate Model
    const modelConfig = (model && model.url && model.name)
        ? model
        : { url: 'https://api.openai.com/v1', name: 'gpt-4o-mini' };

    // Setup headers for SSE
    res.setHeader('Content-Type', 'text/event-stream; charset=utf-8');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');

    // Helper to send events
    const sendEvent = (type, data) => {
        res.write(`data: ${JSON.stringify({ type, ...data })}\n\n`);
    };

    try {
        // Step 1: Router
        sendEvent('status', { content: 'üß≠ ƒêang ph√¢n t√≠ch ng·ªØ c·∫£nh & c√¢u h·ªèi...' });

        // Prepare History & Context
        let history = [];
        let processingMessage = message;
        if (userId && conversationId) {
            history = await getChatHistory(userId, conversationId);
            if (history.length > 0) {
                processingMessage = await rewriteQuery(message, history, modelConfig);
            }
        }

        const { intent, reasoning } = await classifyIntent(processingMessage, modelConfig);
        sendEvent('status', { content: `üîç Intent detected: ${intent}` });

        let reply = '';
        let reasoningDetail = [`Intent: ${intent}`];
        let chunksUsed = [];
        let metadata = {};

        // Case 1: Greeting
        if (intent === INTENTS.GREETING) {
            sendEvent('status', { content: 'üëã ƒêang so·∫°n c√¢u tr·∫£ l·ªùi...' });
            const directReply = await callLLM(modelConfig, [
                { role: 'system', content: "B·∫°n l√† tr·ª£ l√Ω AI th√¢n thi·ªán. H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn." },
                ...history.slice(-4),
                { role: 'user', content: message }
            ]);
            reply = directReply;
            sendEvent('text', { content: reply });
        }

        // Case 2: Live Search
        else if (intent === INTENTS.LIVE_SEARCH) {
            sendEvent('status', { content: 'üåç ƒêang t√¨m ki·∫øm tr√™n internet...' });
            const searchContext = await performWebSearch(processingMessage);

            sendEvent('status', { content: 'üìù ƒêang t·ªïng h·ª£p th√¥ng tin...' });
            const systemPrompt = `B·∫°n l√† tr·ª£ l√Ω c·∫≠p nh·∫≠t tin t·ª©c. Tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin sau:\n${searchContext}`;

            reply = await callLLM(modelConfig, [
                { role: 'system', content: systemPrompt },
                ...history.slice(-4),
                { role: 'user', content: message }
            ]);
            // Note: callLLM hi·ªán t·∫°i ch∆∞a support stream token, n√™n ta g·ª≠i c·∫£ chunk text
            sendEvent('text', { content: reply });
        }

        // Case 3: Knowledge RAG (Simplified for Stream Demo)
        else if (intent === INTENTS.KNOWLEDGE) {
            sendEvent('status', { content: 'üß† ƒêang tra c·ª©u d·ªØ li·ªáu n·ªôi b·ªô...' });
            // Reuse existing RAG logic here if needed, or simplified version
            const questionEmbedding = await getEmbedding(processingMessage);
            const rawChunks = await multiStageRetrieval(questionEmbedding, processingMessage, 5);
            chunksUsed = rawChunks.map(c => ({
                id: c.id,
                title: c.title,
                content: c.content,
                score: c.score,
                source: c.source_type || 'vector',
                stage: c.retrieval_stage || 'retrieval'
            }));

            if (rawChunks.length === 0) {
                reply = "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu.";
            } else {
                sendEvent('status', { content: 'üí° ƒêang suy lu·∫≠n...' });
                const fusedContext = fuseContext(rawChunks, [], processingMessage);
                reply = await callLLM(modelConfig, [
                    { role: 'system', content: "Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n context sau:\n" + fusedContext },
                    ...history.slice(-6),
                    { role: 'user', content: message }
                ]);
            }
            sendEvent('text', { content: reply });
        }

        // Case 4: Off Topic
        else {
            reply = "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.";
            sendEvent('text', { content: reply });
        }

        // Save to DB and get Conversation ID
        let finalConversationId = conversationId;
        if (userId) {
            finalConversationId = await getOrCreateConversationId(userId, conversationId);

            // Determine title if new conversation
            let conversationTitle = null;
            if (!conversationId) { // Only check if new conversation
                const [existingMessages] = await pool.execute(
                    'SELECT COUNT(*) as count FROM user_questions WHERE user_id = ? AND conversation_id = ?',
                    [userId, finalConversationId]
                );
                if (existingMessages[0].count === 0) {
                    conversationTitle = message.trim().substring(0, 50);
                }
            }

            const metadata = {
                processing_time: 0, // Placeholder
                model: modelConfig.name,
                total_chunks: chunksUsed.length,
                intent: intent,
                source: 'stream'
            };

            await pool.execute(
                'INSERT INTO user_questions (user_id, conversation_id, conversation_title, question, bot_reply, is_answered, metadata) VALUES (?, ?, ?, ?, ?, ?, ?)',
                [userId, finalConversationId, conversationTitle, message, reply, true, JSON.stringify(metadata)]
            );

            // Track usage
            await trackUsage(userId, 'stream_chat', { tokens: reply.length / 4 });
        }

        // Finalize
        sendEvent('done', {
            reply,
            reasoning_steps: reasoningDetail,
            chunks_used: chunksUsed,
            conversationId: finalConversationId
        });

    } catch (error) {
        console.error('Stream Error:', error);
        sendEvent('error', { message: 'ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω.' });
    } finally {
        res.end();
    }
}
