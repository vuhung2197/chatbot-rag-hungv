import pool from '../../../db.js';
import { getEmbedding } from '../../../services/embeddingVector.js';
import { retrieveTopChunks } from '../../../services/rag_retrieve.js';
import { hashQuestion } from '../../../utils/hash.js';
import { StatusCodes } from 'http-status-codes';
import '../../../bootstrap/env.js';
import axios from 'axios';
// Temporary import from old usageController location
import { trackUsage } from '../usage/usage.controller.js';
import { getOrCreateConversationId } from './conversation.controller.js';
import {
    multiStageRetrieval,
    semanticClustering,
    multiHopReasoning,
    fuseContext,
    adaptiveRetrieval,
    rerankContext
} from '../../../services/advancedRAGFixed.js';

// ==================== HELPER FUNCTIONS ====================

/**
 * Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n AI tr·∫£ l·ªùi th√†nh Markdown gi·ªëng ChatGPT.
 */
function toMarkdown(text) {
    if (!text) return '';

    const paragraphs = text.split(/\n{2,}/);
    const firstPara = paragraphs.shift()?.trim();
    let markdown = '';

    // B1: C√¢u ƒë·∫ßu ti√™n in ƒë·∫≠m
    if (firstPara) {
        const sentences = firstPara.split(/(?<=\.)\s+/);
        const firstSentence = sentences.shift();
        markdown += `**${firstSentence.trim()}**\n\n`;
        if (sentences.length) {
            markdown += `${sentences.join(' ')}\n\n`;
        }
    }

    // B2: Duy·ªát c√°c ƒëo·∫°n c√≤n l·∫°i
    for (let para of paragraphs) {
        para = para.trim();
        if (!para) continue;

        const isList =
            para.startsWith('- ') ||
            para.startsWith('* ') ||
            /^[‚Ä¢\-+]\s/.test(para) ||
            (/(,|\.)\s/.test(para) && para.length < 200);

        if (isList) {
            const points = para
                .split(/(?:^|\n)[‚Ä¢\-+*]?\s*/)
                .map((p) => p.trim())
                .filter((p) => p.length > 0);
            points.forEach((point) => {
                markdown += `- ${point}\n`;
            });
            markdown += '\n';
        } else {
            markdown += `${para}\n\n`;
        }
    }

    return markdown.trim();
}

/**
 * Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n AI tr·∫£ l·ªùi th√†nh Markdown v·ªõi c·∫•u tr√∫c t·ªët h∆°n (Advanced)
 */
function toAdvancedMarkdown(text) {
    if (!text) return '';

    const paragraphs = text.split(/\n{2,}/);
    let markdown = '';

    for (const para of paragraphs) {
        const trimmed = para.trim();
        if (!trimmed) continue;

        // Detect headers
        if (trimmed.match(/^#{1,6}\s/)) {
            markdown += `${trimmed}\n\n`;
            continue;
        }

        // Detect lists
        if (trimmed.startsWith('- ') || trimmed.startsWith('* ') || /^[‚Ä¢\-+]\s/.test(trimmed)) {
            const points = trimmed
                .split(/(?:^|\n)[‚Ä¢\-+*]?\s*/)
                .map(p => p.trim())
                .filter(p => p.length > 0);

            points.forEach(point => {
                markdown += `- ${point}\n`;
            });
            markdown += '\n';
            continue;
        }

        // Detect code blocks
        if (trimmed.startsWith('```')) {
            markdown += `${trimmed}\n\n`;
            continue;
        }

        // Regular paragraph
        markdown += `${trimmed}\n\n`;
    }

    return markdown.trim();
}

/**
 * ·∫®n th√¥ng tin nh·∫°y c·∫£m
 */
export function maskSensitiveInfo(text, mapping = {}) {
    let counter = 1;
    // S·ªë ƒëi·ªán tho·∫°i
    text = text.replace(/\b\d{2,4}[-\s]?\d{3,4}[-\s]?\d{3,4}\b/g, (match) => {
        const key = `[PHONE_${counter++}]`;
        mapping[key] = match;
        return key;
    });
    // Email
    text = text.replace(
        /\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b/gi,
        (match) => {
            const key = `[EMAIL_${counter++}]`;
            mapping[key] = match;
            return key;
        }
    );
    // ƒê·ªãa ch·ªâ
    text = text.replace(
        /(\d{1,4}\s?[\w\s,./-]+(ƒë∆∞·ªùng|ph·ªë|t√≤a nh√†)[^\n,.]*)/gi,
        (match) => {
            const key = `[ADDR_${counter++}]`;
            mapping[key] = match;
            return key;
        }
    );
    return text;
}

/**
 * Kh√¥i ph·ª•c th√¥ng tin nh·∫°y c·∫£m
 */
export function unmaskSensitiveInfo(text, mapping) {
    for (const [key, value] of Object.entries(mapping)) {
        text = text.replaceAll(key, value);
    }
    return text;
}

/**
 * G·ªçi API m√¥ h√¨nh ng√¥n ng·ªØ
 */
export async function callLLM(model, messages, _temperature = 0.2, _maxTokens = 512) {
    if (!model || !model.url || !model.name) {
        throw new Error('Invalid model configuration: missing url or name');
    }

    const baseUrl = model.url;
    const nameModel = model.name;
    const temperatureModel = model.temperature !== undefined ? model.temperature : _temperature;
    const maxTokensModel = model.maxTokens !== undefined ? model.maxTokens : _maxTokens;

    const normalizedUrl = baseUrl.endsWith('/') ? baseUrl.slice(0, -1) : baseUrl;
    const fullUrl = `${normalizedUrl}/chat/completions`;

    console.log('üîó Calling LLM:', {
        url: fullUrl,
        model: nameModel,
        temperature: temperatureModel,
        max_tokens: maxTokensModel,
        messages_count: messages.length
    });

    try {
        const response = await axios.post(
            fullUrl,
            {
                model: nameModel,
                messages,
                temperature: temperatureModel,
                max_tokens: maxTokensModel,
            },
            {
                headers: { 'Content-Type': 'application/json' },
                timeout: 180000,
            }
        );

        const content = response.data.choices[0].message.content.trim();
        console.log('‚úÖ LLM response received successfully');
        return content;
    } catch (error) {
        console.error('‚ùå LLM call error:', {
            message: error.message,
            response: error.response?.data,
            status: error.response?.status,
        });
        throw new Error(`LLM API Error: ${error.message} - ${error.response?.data ? JSON.stringify(error.response.data) : ''}`);
    }
}

/**
 * Log unanswered questions
 */
async function logUnanswered(question) {
    try {
        const hash = hashQuestion(question);
        const [rows] = await pool.execute(
            'SELECT 1 FROM unanswered_questions WHERE hash = ? LIMIT 1',
            [hash]
        );
        if (rows.length === 0) {
            await pool.execute(
                'INSERT INTO unanswered_questions (question, hash, created_at) VALUES (?, ?, NOW())',
                [question, hash]
            );
        }
    } catch (e) {
        console.warn('‚ö†Ô∏è Kh√¥ng th·ªÉ ghi log unanswered:', e.message);
    }
}

/**
 * G·ªçi OpenAI ChatGPT (Basic)
 */
export async function askChatGPT(
    question,
    context,
    systemPrompt = 'B·∫°n l√† tr·ª£ l√Ω AI chuy√™n tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.',
    model
) {
    const mapping = {};
    const maskedQuestion = maskSensitiveInfo(question, mapping);

    let prompt = '';
    if (context && context.trim().length > 0) {
        const maskedContext = maskSensitiveInfo(context, mapping);
        prompt = `Th√¥ng tin tham kh·∫£o:\n${maskedContext}\n\nC√¢u h·ªèi: ${maskedQuestion}`;
    } else {
        prompt = maskedQuestion;
    }

    const messages = [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: prompt },
    ];

    let reply = await callLLM(model, messages, 0.2, 512);
    reply = unmaskSensitiveInfo(reply, mapping);

    return reply;
}

/**
 * G·ªçi LLM v·ªõi context n√¢ng cao (Advanced)
 */
async function askAdvancedChatGPT(question, context, systemPrompt, model) {
    const maxContextLength = 6000;
    const truncatedContext = context.length > maxContextLength
        ? `${context.substring(0, maxContextLength)}...`
        : context;

    console.log(`üìù Context size: ${context.length} chars, truncated to: ${truncatedContext.length} chars`);

    const prompt = `# C√¢u h·ªèi: ${question}
 
 # Th√¥ng tin tham kh·∫£o:
 ${truncatedContext}
 
 # H∆∞·ªõng d·∫´n:
 H√£y ph√¢n t√≠ch c√¢u h·ªèi v√† s·ª≠ d·ª•ng th√¥ng tin tham kh·∫£o ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi to√†n di·ªán. 
 K·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn m·ªôt c√°ch logic v√† c√≥ c·∫•u tr√∫c.`;

    const messages = [
        { role: 'system', content: (systemPrompt || '').substring(0, 4000) },
        { role: 'user', content: prompt.substring(0, 8000) }
    ];

    const reply = await callLLM(model, messages, 0.3, 800);
    return reply;
}


// ==================== CONTROLLER FUNCTIONS ====================

/**
 * X·ª≠ l√Ω API chat ch√≠nh s·ª≠ d·ª•ng thu·∫ßn RAG.
 */
export async function chat(req, res) {
    const { message, model, conversationId } = req.body;
    const userId = req.user?.id;

    if (!message)
        return res.status(StatusCodes.BAD_REQUEST).json({ reply: 'No message!' });

    try {
        let context = '';
        let isAnswered = true;
        const systemPrompt = 'B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp, tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c.';

        let embedding;
        try {
            embedding = await getEmbedding(message);
        } catch (error) {
            console.error('‚ùå L·ªói t·∫°o embedding:', error);
            isAnswered = false;
            if (userId) {
                await pool.execute(
                    'INSERT INTO user_questions (user_id, question, is_answered) VALUES (?, ?, ?)',
                    [userId, message, false]
                );
            }
            return res.json({ reply: 'Kh√¥ng th·ªÉ t√≠nh embedding c√¢u h·ªèi!' });
        }

        const chunks = await retrieveTopChunks(embedding);
        if (!chunks.length) {
            isAnswered = false;
            await logUnanswered(message);
            if (userId) {
                await pool.execute(
                    'INSERT INTO user_questions (user_id, question, is_answered) VALUES (?, ?, ?)',
                    [userId, message, false]
                );
            }
            return res.json({
                reply: 'T√¥i ch∆∞a c√≥ ki·∫øn th·ª©c ph√π h·ª£p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.',
            });
        }

        context = chunks
            .map((c) => `Ti√™u ƒë·ªÅ: ${c.title}\nN·ªôi dung: ${c.content}`)
            .join('\n---\n');

        const t0 = Date.now();
        const reply = await askChatGPT(message, context, systemPrompt, model);
        const t1 = Date.now();
        console.log('‚è±Ô∏è Th·ªùi gian g·ªçi OpenAI:', t1 - t0, 'ms');

        if (userId) {
            const finalConversationId = await getOrCreateConversationId(userId, conversationId);
            const [existingMessages] = await pool.execute(
                'SELECT COUNT(*) as count FROM user_questions WHERE user_id = ? AND conversation_id = ?',
                [userId, finalConversationId]
            );

            let conversationTitle = null;
            if (existingMessages[0].count === 0) {
                conversationTitle = message.trim().substring(0, 50);
                if (message.length > 50) conversationTitle += '...';
            }

            const metadata = {
                total_chunks: chunks.length,
                processing_time: t1 - t0,
                model_used: model?.name || 'gpt-4o',
                context_length: context.length,
                chunks_used: chunks.map(c => ({
                    id: c.id,
                    title: c.title,
                    content: c.content.substring(0, 200) + (c.content.length > 200 ? '...' : ''),
                    score: c.score,
                    source: c.source || 'unknown'
                }))
            };

            await pool.execute(
                'INSERT INTO user_questions (user_id, conversation_id, conversation_title, question, bot_reply, is_answered, metadata) VALUES (?, ?, ?, ?, ?, ?, ?)',
                [userId, finalConversationId, conversationTitle, message, reply, isAnswered, JSON.stringify(metadata)]
            );

            await trackUsage(userId, 'query', { tokens: context.length || 0 });

            res.json({
                reply: toMarkdown(reply),
                conversationId: finalConversationId,
                chunks_used: chunks.map(c => ({
                    id: c.id,
                    title: c.title,
                    content: c.content.substring(0, 200) + (c.content.length > 200 ? '...' : ''),
                    score: c.score,
                    source: c.source || 'unknown'
                })),
                metadata: {
                    total_chunks: chunks.length,
                    processing_time: t1 - t0,
                    model_used: model?.name || 'gpt-4o',
                    context_length: context.length
                }
            });
            return;
        }

        res.json({
            reply: toMarkdown(reply),
            chunks_used: chunks.map(c => ({
                id: c.id,
                title: c.title,
                content: c.content.substring(0, 200) + (c.content.length > 200 ? '...' : ''),
                score: c.score,
                source: c.source || 'unknown'
            })),
            metadata: {
                total_chunks: chunks.length,
                processing_time: t1 - t0,
                model_used: model?.name || 'gpt-4o',
                context_length: context.length
            }
        });
    } catch (err) {
        console.error('‚ùå L·ªói x·ª≠ l√Ω:', err);
        res.json({ reply: 'Bot ƒëang b·∫≠n, vui l√≤ng th·ª≠ l·∫°i sau!' });
    }
}

/**
 * API l·∫•y l·ªãch s·ª≠ chat g·∫ßn nh·∫•t (gi·ªØ nguy√™n legacy endpoint).
 */
export async function history(req, res) {
    const userId = req.user?.id;

    if (!userId)
        return res
            .status(StatusCodes.UNAUTHORIZED)
            .json({ error: 'Ch∆∞a ƒëƒÉng nh·∫≠p' });

    try {
        const [rows] = await pool.execute(
            `SELECT id, question, bot_reply, is_answered, created_at 
       FROM user_questions 
       WHERE user_id = ? 
       ORDER BY created_at DESC 
       LIMIT 50`,
            [userId]
        );
        res.json(rows);
    } catch (err) {
        console.error('‚ùå L·ªói khi l·∫•y l·ªãch s·ª≠ c√¢u h·ªèi:', err);
        res.status(500).json({ error: 'L·ªói server' });
    }
}

/**
 * Advanced Chat API v·ªõi Multi-Chunk Reasoning
 */
export async function advancedChat(req, res) {
    const { message, model, conversationId } = req.body;
    const userId = req.user?.id;

    if (!message) {
        return res.status(StatusCodes.BAD_REQUEST).json({
            reply: 'No message!',
            reasoning_steps: [],
            chunks_used: []
        });
    }

    if (!model || !model.url || !model.name) {
        console.error('‚ùå Invalid model configuration:', model);
        return res.status(StatusCodes.BAD_REQUEST).json({
            reply: 'Invalid model configuration!',
            reasoning_steps: [],
            chunks_used: []
        });
    }

    try {
        console.log('üß† Advanced RAG processing:', message);
        console.log('üìã Model config:', model);

        let questionEmbedding;
        try {
            questionEmbedding = await getEmbedding(message);
        } catch (error) {
            console.error('‚ùå L·ªói t·∫°o embedding:', error);
            return res.json({
                reply: 'Kh√¥ng th·ªÉ x·ª≠ l√Ω c√¢u h·ªèi n√†y!',
                reasoning_steps: [],
                chunks_used: []
            });
        }

        const retrievalParams = await adaptiveRetrieval(message, questionEmbedding);
        console.log('üìä Retrieval params:', retrievalParams);

        const allChunks = await multiStageRetrieval(
            questionEmbedding,
            message,
            retrievalParams.maxChunks
        );

        if (allChunks.length === 0) {
            await logUnanswered(message);
            return res.json({
                reply: 'T√¥i ch∆∞a c√≥ ki·∫øn th·ª©c ph√π h·ª£p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.',
                reasoning_steps: ['Kh√¥ng t√¨m th·∫•y chunks ph√π h·ª£p'],
                chunks_used: []
            });
        }

        console.log(`üìö Retrieved ${allChunks.length} chunks`);

        let clusters = [];
        try {
            clusters = await semanticClustering(allChunks, questionEmbedding);
        } catch (error) {
            console.error('‚ùå Error in semantic clustering:', error);
            clusters = [allChunks];
        }

        let reasoningChains = [];
        if (retrievalParams.useMultiHop) {
            try {
                reasoningChains = await multiHopReasoning(
                    allChunks.slice(0, 5),
                    questionEmbedding,
                    message
                );
            } catch (error) {
                console.error('‚ùå Error in multi-hop reasoning:', error);
                reasoningChains = [];
            }
        }

        let rerankedChunks = allChunks;
        try {
            rerankedChunks = rerankContext(allChunks, questionEmbedding, message);
        } catch (error) {
            console.error('‚ùå Error in context re-ranking:', error);
        }

        let fusedContext = '';
        try {
            fusedContext = fuseContext(rerankedChunks, reasoningChains, message);
        } catch (error) {
            console.error('‚ùå Error in context fusion:', error);
            fusedContext = rerankedChunks.map(c => `**${c.title}**: ${c.content}`).join('\n\n');
        }

        const systemPrompt = `B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp v·ªõi kh·∫£ nƒÉng ph√¢n t√≠ch v√† k·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn.
H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
1. Ph√¢n t√≠ch c√¢u h·ªèi ƒë·ªÉ x√°c ƒë·ªãnh c√°c kh√≠a c·∫°nh c·∫ßn tr·∫£ l·ªùi
2. K·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn m·ªôt c√°ch logic
3. T·∫°o c√¢u tr·∫£ l·ªùi c√≥ c·∫•u tr√∫c r√µ r√†ng v·ªõi c√°c ph·∫ßn:
   - T√≥m t·∫Øt ch√≠nh
   - Chi ti·∫øt t·ª´ng kh√≠a c·∫°nh
   - K·∫øt lu·∫≠n v√† li√™n k·∫øt
4. S·ª≠ d·ª•ng markdown ƒë·ªÉ format c√¢u tr·∫£ l·ªùi
5. N·∫øu th√¥ng tin kh√¥ng ƒë·ªß, h√£y n√≥i r√µ v√† ƒë·ªÅ xu·∫•t h∆∞·ªõng t√¨m hi·ªÉu th√™m`;

        const t0 = Date.now();
        let reply = '';
        try {
            const timeoutPromise = new Promise((_, reject) =>
                setTimeout(() => reject(new Error('LLM call timeout')), 180000)
            );

            const llmPromise = askAdvancedChatGPT(message, fusedContext, systemPrompt, model);
            reply = await Promise.race([llmPromise, timeoutPromise]);
        } catch (error) {
            console.error('‚ùå Error in LLM call for Advanced RAG:', error);

            if (error.message && error.message.includes('LLM API Error')) {
                reply = `L·ªói k·∫øt n·ªëi v·ªõi model: ${error.message}`;
            } else if (error.message && error.message.includes('timeout')) {
                reply = 'Model m·∫•t qu√° nhi·ªÅu th·ªùi gian ƒë·ªÉ ph·∫£n h·ªìi. Vui l√≤ng th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi ng·∫Øn g·ªçn h∆°n.';
            } else {
                reply = 'Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë khi x·ª≠ l√Ω c√¢u h·ªèi ph·ª©c t·∫°p n√†y. Vui l√≤ng th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi ƒë∆°n gi·∫£n h∆°n.';
            }
        }

        const t1 = Date.now();
        console.log('‚è±Ô∏è Advanced RAG processing time:', t1 - t0, 'ms');

        const reasoningSteps = [
            `Retrieved ${allChunks.length} chunks using multi-stage retrieval`,
            `Created ${clusters.length} semantic clusters`,
            `Generated ${reasoningChains.length} reasoning chains`,
            `Fused context with ${fusedContext.length} characters`,
            `Generated response using advanced RAG with model ${model.name}`
        ];

        if (userId) {
            const finalConversationId = await getOrCreateConversationId(userId, conversationId);
            const [existingMessages] = await pool.execute(
                'SELECT COUNT(*) as count FROM user_questions WHERE user_id = ? AND conversation_id = ?',
                [userId, finalConversationId]
            );

            let conversationTitle = null;
            if (existingMessages[0].count === 0) {
                conversationTitle = message.trim().substring(0, 50);
                if (message.length > 50) conversationTitle += '...';
            }

            const metadata = {
                total_chunks: allChunks.length,
                clusters: clusters.length,
                reasoning_chains: reasoningChains.length,
                processing_time: t1 - t0,
                model_used: model.name,
                context_length: fusedContext.length,
                reasoning_steps: reasoningSteps,
                chunks_used: rerankedChunks.map(c => ({
                    id: c.id,
                    title: c.title,
                    content: c.content.substring(0, 200) + (c.content.length > 200 ? '...' : ''),
                    score: c.final_score || c.score,
                    stage: c.retrieval_stage,
                    source: c.source || 'unknown',
                    chunk_index: c.chunk_index || 0
                }))
            };

            await pool.execute(
                'INSERT INTO user_questions (user_id, conversation_id, conversation_title, question, bot_reply, is_answered, metadata) VALUES (?, ?, ?, ?, ?, ?, ?)',
                [userId, finalConversationId, conversationTitle, message, reply, true, JSON.stringify(metadata)]
            );

            await trackUsage(userId, 'advanced_rag', { tokens: fusedContext.length || 0 });

            res.json({
                reply: toAdvancedMarkdown(reply),
                conversationId: finalConversationId,
                reasoning_steps: reasoningSteps,
                chunks_used: rerankedChunks.map(c => ({
                    id: c.id,
                    title: c.title,
                    content: c.content.substring(0, 200) + (c.content.length > 200 ? '...' : ''),
                    score: c.final_score || c.score,
                    stage: c.retrieval_stage,
                    source: c.source || 'unknown',
                    chunk_index: c.chunk_index || 0
                })),
                metadata
            });
            return;
        }

        res.json({
            reply: toAdvancedMarkdown(reply),
            reasoning_steps: reasoningSteps,
            chunks_used: rerankedChunks.map(c => ({
                id: c.id,
                title: c.title,
                content: c.content.substring(0, 200) + (c.content.length > 200 ? '...' : ''),
                score: c.final_score || c.score,
                stage: c.retrieval_stage,
                source: c.source || 'unknown',
                chunk_index: c.chunk_index || 0
            })),
            metadata: {
                total_chunks: allChunks.length,
                clusters: clusters.length,
                reasoning_chains: reasoningChains.length,
                processing_time: t1 - t0,
                model_used: model.name,
                context_length: fusedContext.length
            }
        });

    } catch (err) {
        console.error('‚ùå Advanced RAG error:', err);
        res.json({
            reply: 'Bot ƒëang g·∫∑p s·ª± c·ªë v·ªõi c√¢u h·ªèi ph·ª©c t·∫°p n√†y. Vui l√≤ng th·ª≠ l·∫°i!',
            reasoning_steps: ['Error in advanced processing'],
            chunks_used: []
        });
    }
}

/**
 * Get advanced RAG statistics
 */
export async function getAdvancedRAGStats(req, res) {
    try {
        const [stats] = await pool.execute(`
      SELECT 
        COUNT(*) as total_questions,
        AVG(CAST(metadata->>'total_chunks' AS NUMERIC)) as avg_chunks,
        AVG(CAST(metadata->>'processing_time' AS NUMERIC)) as avg_processing_time,
        COUNT(CASE WHEN CAST(metadata->>'reasoning_chains' AS NUMERIC) > 0 THEN 1 END) as complex_questions
      FROM user_questions 
      WHERE metadata IS NOT NULL
    `);

        res.json({
            success: true,
            stats: stats[0]
        });
    } catch (err) {
        console.error('‚ùå L·ªói get stats:', err);
        res.status(500).json({ success: false, error: err.message });
    }
}
