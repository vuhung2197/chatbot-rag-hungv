import pool from '#db';
import { getEmbedding } from '#services/embeddingVector.js';
import { hashQuestion } from '#utils/hash.js';
import conversationService from './conversation.service.js';
import usageService from '#modules/usage/services/usage.service.js';
import {
    multiStageRetrieval,
    semanticClustering,
    multiHopReasoning,
    fuseContext,
    adaptiveRetrieval,
    rerankContext
} from '#services/advancedRAGFixed.js';
import { callLLM } from '#services/llmService.js';
import { performWebSearch } from '#services/webSearch.service.js';
import { classifyIntent, INTENTS } from '#services/intentRouter.js';

// ==================== HELPER FUNCTIONS ====================

/**
 * Chuyá»ƒn Ä‘á»•i vÄƒn báº£n AI tráº£ lá»i thÃ nh Markdown giá»‘ng ChatGPT. (Deprecated/Simple)
 */
function toMarkdown(text) {
    if (!text) return '';
    // ... logic (simplified or full)
    // For now using the advanced one as primary
    return toAdvancedMarkdown(text);
}

/**
 * Chuyá»ƒn Ä‘á»•i vÄƒn báº£n AI tráº£ lá»i thÃ nh Markdown vá»›i cáº¥u trÃºc tá»‘t hÆ¡n (Advanced)
 */
function toAdvancedMarkdown(text) {
    if (!text) return '';
    const paragraphs = text.split(/\n{2,}/);
    let markdown = '';
    for (const para of paragraphs) {
        const trimmed = para.trim();
        if (!trimmed) continue;
        if (trimmed.match(/^#{1,6}\s/)) { markdown += `${trimmed}\n\n`; continue; }
        if (trimmed.startsWith('- ') || trimmed.startsWith('* ') || /^[â€¢\-+]\s/.test(trimmed)) {
            const points = trimmed.split(/(?:^|\n)[â€¢\-+*]?\s*/).map(p => p.trim()).filter(p => p.length > 0);
            points.forEach(point => { markdown += `- ${point}\n`; });
            markdown += '\n';
            continue;
        }
        if (trimmed.startsWith('```')) { markdown += `${trimmed}\n\n`; continue; }
        markdown += `${trimmed}\n\n`;
    }
    return markdown.trim();
}

/**
 * áº¨n thÃ´ng tin nháº¡y cáº£m
 */
function maskSensitiveInfo(text, mapping = {}) {
    let counter = 1;
    // Phone
    text = text.replace(/\b\d{2,4}[-\s]?\d{3,4}[-\s]?\d{3,4}\b/g, (match) => {
        const key = `[PHONE_${counter++}]`;
        mapping[key] = match;
        return key;
    });
    // Email
    text = text.replace(/\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b/gi, (match) => {
        const key = `[EMAIL_${counter++}]`;
        mapping[key] = match;
        return key;
    });
    return text;
}

/**
 * KhÃ´i phá»¥c thÃ´ng tin nháº¡y cáº£m
 */
function unmaskSensitiveInfo(text, mapping) {
    for (const [key, value] of Object.entries(mapping)) {
        text = text.replaceAll(key, value);
    }
    return text;
}

// ==================== WEB SEARCH HELPERS ====================

class ChatService {
    async logUnanswered(question) {
        try {
            const hash = hashQuestion(question);
            const [rows] = await pool.execute(
                'SELECT 1 FROM unanswered_questions WHERE hash = ? LIMIT 1',
                [hash]
            );
            if (rows.length === 0) {
                await pool.execute(
                    'INSERT INTO unanswered_questions (question, hash, created_at) VALUES (?, ?, NOW())',
                    [question, hash]
                );
            }
        } catch (e) {
            console.warn('âš ï¸ KhÃ´ng thá»ƒ ghi log unanswered:', e.message);
        }
    }

    async getChatHistory(userId, conversationId, limit = 6) {
        if (!conversationId || !userId) return [];
        try {
            const [rows] = await pool.execute(
                `SELECT question, bot_reply FROM user_questions 
             WHERE user_id = ? AND conversation_id = ? 
             ORDER BY created_at DESC LIMIT ?`,
                [userId, conversationId, limit]
            );
            const history = [];
            for (let i = rows.length - 1; i >= 0; i--) {
                const row = rows[i];
                if (row.question) history.push({ role: 'user', content: row.question });
                if (row.bot_reply) history.push({ role: 'assistant', content: row.bot_reply });
            }
            return history;
        } catch (e) {
            console.warn('âš ï¸ Filed to fetch history:', e.message);
            return [];
        }
    }

    async rewriteQuery(message, history, modelConfig) {
        if (!history || history.length === 0) return message;
        const historyText = history.slice(-4).map(h => `${h.role === 'user' ? 'User' : 'AI'}: ${h.content}`).join('\n');
        const systemPrompt = `Báº¡n lÃ  chuyÃªn gia vá» ngÃ´n ngá»¯. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  viáº¿t láº¡i cÃ¢u há»i follow-up cá»§a ngÆ°á»i dÃ¹ng thÃ nh má»™t cÃ¢u há»i Ä‘á»™c láº­p (Standalone Question) Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh, dá»±a trÃªn lá»‹ch sá»­ há»™i thoáº¡i.
- GIá»® NGUYÃŠN ná»™i dung cá»‘t lÃµi cá»§a cÃ¢u há»i.
- THAY THáº¾ cÃ¡c Ä‘áº¡i tá»« thay tháº¿ (nÃ³, anh áº¥y, cÃ¡i Ä‘Ã³...) báº±ng danh tá»« cá»¥ thá»ƒ tá»« lá»‹ch sá»­.
- Náº¾U cÃ¢u há»i Ä‘Ã£ rÃµ rÃ ng, giá»¯ nguyÃªn.
- CHá»ˆ TRáº¢ Vá»€ CÃ‚U Há»ŽI ÄÃƒ VIáº¾T Láº I. KHÃ”NG tráº£ lá»i cÃ¢u há»i.`;

        try {
            const rewritten = await callLLM(modelConfig, [
                { role: 'system', content: systemPrompt },
                { role: 'user', content: `Lá»‹ch sá»­ há»™i thoáº¡i:\n${historyText}\n\nCÃ¢u há»i hiá»‡n táº¡i: ${message}\n\nViáº¿t láº¡i:` }
            ], 0.3, 200);
            return rewritten.trim().replace(/^['"]|['"]$/g, '');
        } catch (e) {
            console.error('Rewrite query failed:', e.message);
            return message;
        }
    }

    async processChat({ userId, message, model, conversationId }) {
        if (!message) throw new Error('No message provided');

        // Validate Model Config
        const modelConfig = (model && model.url && model.name)
            ? model
            : { url: 'https://api.openai.com/v1', name: 'gpt-4o-mini' };

        let history = [];
        let processingMessage = message;

        if (userId && conversationId) {
            history = await this.getChatHistory(userId, conversationId);
            if (history.length > 0) {
                processingMessage = await this.rewriteQuery(message, history, modelConfig);
            }
        }

        // Router
        const { intent, reasoning } = await classifyIntent(processingMessage, modelConfig);
        console.log(`ðŸ§­ Intent: ${intent} | ${reasoning}`);

        // Handle OFF_TOPIC
        if (intent === INTENTS.OFF_TOPIC) {
            return {
                reply: "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ tháº£o luáº­n vá» chá»§ Ä‘á» nÃ y do cÃ¡c quy Ä‘á»‹nh vá» an toÃ n ná»™i dung.",
                reasoning_steps: [`Intent: OFF_TOPIC (${reasoning})`, 'Action: Refusal'],
                chunks_used: []
            };
        }

        // Handle GREETING
        if (intent === INTENTS.GREETING) {
            const directSystemPrompt = "Báº¡n lÃ  trá»£ lÃ½ AI thÃ¢n thiá»‡n. HÃ£y tráº£ lá»i ngÆ°á»i dÃ¹ng má»™t cÃ¡ch tá»± nhiÃªn, lá»‹ch sá»± vÃ  ngáº¯n gá»n.";
            const messages = [
                { role: 'system', content: directSystemPrompt },
                ...history.slice(-4),
                { role: 'user', content: message }
            ];
            const directReply = await callLLM(modelConfig, messages, 0.7, 200);
            return {
                reply: directReply,
                reasoning_steps: [`Intent: GREETING (${reasoning})`, 'Action: Direct Chat (No RAG)'],
                chunks_used: []
            };
        }

        // Handle LIVE_SEARCH
        if (intent === INTENTS.LIVE_SEARCH) {
            const t0 = Date.now();
            const searchResult = await performWebSearch(processingMessage);
            const { context: searchContext, sources: webSources } = searchResult;

            const systemPrompt = `Báº¡n lÃ  má»™t trá»£ lÃ½ cáº­p nháº­t tin tá»©c thÃ´ng minh. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng dá»±a trÃªn káº¿t quáº£ tÃ¬m kiáº¿m web má»›i nháº¥t Ä‘Æ°á»£c cung cáº¥p.\nThá»i gian hiá»‡n táº¡i: ${new Date().toLocaleString('vi-VN')}\n\nYÃªu cáº§u:\n1. Tráº£ lá»i chÃ­nh xÃ¡c, ngáº¯n gá»n.\n2. DáºªN NGUá»’N (Link URL) á»Ÿ cuá»‘i cÃ¢u tráº£ lá»i dáº¡ng [Title](URL).\n3. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, hÃ£y thÃ nh tháº­t nÃ³i khÃ´ng biáº¿t.\n4. TrÃ¬nh bÃ y Ä‘áº¹p báº±ng Markdown.`;

            const replyRaw = await callLLM(modelConfig, [
                { role: 'system', content: systemPrompt },
                ...history.slice(-4),
                { role: 'user', content: `# CÃ¢u há»i: ${message}\n\n${searchContext}` }
            ], 0.4, 800);

            const reply = toAdvancedMarkdown(replyRaw);
            const processTime = Date.now() - t0;
            const reasoningSteps = [
                `Intent: LIVE_SEARCH (${reasoning})`,
                `Performed Web Search via Tavily AI`,
                `Synthesized answer from ${webSources.length} web results`,
                `Processing time: ${processTime}ms`
            ];

            if (userId) {
                const finalConversationId = await conversationService.getOrCreateConversationId(userId, conversationId);
                await this.saveChat(userId, finalConversationId, message, reply, { processing_time: processTime, model: modelConfig.name, intent: intent, source: 'web_search' });
                await usageService.trackUsage(userId, 'web_search', { tokens: searchContext.length });
                // Gap 4: Include web_sources + source_type in response
                return { reply, conversationId: finalConversationId, chunks_used: [], reasoning_steps: reasoningSteps, source_type: 'web_search', web_sources: webSources };
            }
            return { reply, chunks_used: [], reasoning_steps: reasoningSteps, source_type: 'web_search', web_sources: webSources };
        }

        // Handle KNOWLEDGE (RAG)
        const t0 = Date.now();
        const questionEmbedding = await getEmbedding(processingMessage);
        const retrievalParams = await adaptiveRetrieval(processingMessage, questionEmbedding);

        const rawChunks = await multiStageRetrieval(
            questionEmbedding,
            processingMessage,
            retrievalParams.maxChunks
        );

        let finalChunks = [];
        try {
            finalChunks = await rerankContext(rawChunks, questionEmbedding, processingMessage);
        } catch (error) {
            console.error('âŒ Re-ranking Error:', error);
            finalChunks = rawChunks;
        }

        if (finalChunks.length === 0) {
            await this.logUnanswered(message);

            // Fallback: Thá»­ tÃ¬m trÃªn Web thay vÃ¬ bá» cuá»™c
            console.log('ðŸ“­ KB returned 0 chunks, falling back to Web Search...');
            {
                try {
                    const searchResult = await performWebSearch(processingMessage);
                    const { context: searchContext, sources: webSources } = searchResult;
                    if (searchContext && !searchContext.includes('chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh') && !searchContext.includes('gáº·p lá»—i')) {
                        const fallbackPrompt = `Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh. CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng khÃ´ng tÃ¬m tháº¥y trong cÆ¡ sá»Ÿ dá»¯ liá»‡u ná»™i bá»™, nÃªn báº¡n sáº½ tráº£ lá»i dá»±a trÃªn káº¿t quáº£ tÃ¬m kiáº¿m web.
Thá»i gian hiá»‡n táº¡i: ${new Date().toLocaleString('vi-VN')}

YÃªu cáº§u:
1. Tráº£ lá»i chÃ­nh xÃ¡c, ngáº¯n gá»n.
2. DáºªN NGUá»’N (Link URL) á»Ÿ cuá»‘i cÃ¢u tráº£ lá»i dáº¡ng [Title](URL).
3. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, hÃ£y thÃ nh tháº­t nÃ³i khÃ´ng biáº¿t.
4. TrÃ¬nh bÃ y Ä‘áº¹p báº±ng Markdown.
5. LÆ°u Ã½: Káº¿t quáº£ nÃ y tá»« internet, KHÃ”NG pháº£i tá»« tÃ i liá»‡u ná»™i bá»™.`;

                        const replyRaw = await callLLM(modelConfig, [
                            { role: 'system', content: fallbackPrompt },
                            ...history.slice(-4),
                            { role: 'user', content: `# CÃ¢u há»i: ${message}\n\n${searchContext}` }
                        ], 0.4, 800);

                        const reply = toAdvancedMarkdown(replyRaw);
                        const processTime = Date.now() - t0;
                        const reasoningSteps = [
                            `Intent: KNOWLEDGE (${reasoning})`,
                            `Retrieval returned 0 relevant chunks from KB`,
                            `Fallback: Performed Web Search via Tavily AI`,
                            `Synthesized answer from ${webSources.length} web results`,
                            `Processing time: ${processTime}ms`
                        ];

                        if (userId) {
                            const finalConversationId = await conversationService.getOrCreateConversationId(userId, conversationId);
                            await this.saveChat(userId, finalConversationId, message, reply, {
                                processing_time: processTime, model: modelConfig.name,
                                intent: 'KNOWLEDGE_FALLBACK_WEB', source: 'kb_fallback_web'
                            });
                            await usageService.trackUsage(userId, 'web_search', { tokens: searchContext.length });
                            return { reply, conversationId: finalConversationId, chunks_used: [], reasoning_steps: reasoningSteps, source_type: 'kb_fallback_web', web_sources: webSources };
                        }
                        return { reply, chunks_used: [], reasoning_steps: reasoningSteps, source_type: 'kb_fallback_web', web_sources: webSources };
                    }
                } catch (fallbackError) {
                    console.warn('âš ï¸ Web Search fallback failed:', fallbackError.message);
                }
            }

            // Náº¿u fallback cÅ©ng tháº¥t báº¡i â†’ tráº£ lá»i gá»‘c
            return {
                reply: 'TÃ´i chÆ°a cÃ³ Ä‘á»§ thÃ´ng tin trong cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y chÃ­nh xÃ¡c.',
                reasoning_steps: ['Retrieval returned 0 relevant chunks', 'Web Search fallback also failed'],
                chunks_used: []
            };
        }

        // Context Synthesis
        let clusters = [], reasoningChains = [];
        if (retrievalParams.useMultiHop) {
            try {
                const results = await Promise.all([
                    semanticClustering(finalChunks, questionEmbedding),
                    multiHopReasoning(finalChunks.slice(0, 5), questionEmbedding, processingMessage)
                ]);
                clusters = results[0];
                reasoningChains = results[1];
            } catch (e) { console.warn('Advanced synthesis skipped:', e); }
        }

        const fusedContext = fuseContext(finalChunks, reasoningChains, processingMessage);
        const systemPrompt = `Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn nghiá»‡p. HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p dÆ°á»›i Ä‘Ã¢y.\nNáº¿u thÃ´ng tin khÃ´ng cÃ³ trong ngá»¯ cáº£nh, hÃ£y nÃ³i "TÃ´i khÃ´ng biáº¿t".\nLuÃ´n trÃ­ch dáº«n nguá»“n (náº¿u cÃ³ thá»ƒ) vÃ  trÃ¬nh bÃ y máº¡ch láº¡c báº±ng Markdown.\n\n---\n${fusedContext}\n---`;

        let reply = '';
        try {
            const replyRaw = await callLLM(modelConfig, [
                { role: 'system', content: systemPrompt },
                ...history.slice(-6),
                { role: 'user', content: message }
            ], 0.3, 1000);
            reply = toAdvancedMarkdown(replyRaw);
        } catch (error) {
            console.error('âŒ LLM Generation Error:', error);
            reply = "Xin lá»—i, Ä‘Ã£ xáº£y ra lá»—i khi táº¡o cÃ¢u tráº£ lá»i.";
        }

        const processTime = Date.now() - t0;
        const reasoningSteps = [
            `Intent: ${intent}`,
            `Retrieved ${rawChunks.length} chunks (Hybrid Search)`,
            `Selected ${finalChunks.length} chunks after Re-ranking`,
            `Processing time: ${processTime}ms`
        ];

        const chunksForClient = finalChunks.map(c => ({
            id: c.id,
            title: c.title,
            content: c.content,
            score: c.final_score || c.score,
            source: c.source_type || 'unknown'
        }));

        if (userId) {
            const finalConversationId = await conversationService.getOrCreateConversationId(userId, conversationId);
            const metadata = {
                processing_time: processTime,
                model: modelConfig.name,
                total_chunks: finalChunks.length,
                intent: intent
            };
            await this.saveChat(userId, finalConversationId, message, reply, metadata);
            await usageService.trackUsage(userId, 'advanced_rag', { tokens: fusedContext.length });
            return { reply, conversationId: finalConversationId, chunks_used: chunksForClient, reasoning_steps: reasoningSteps };
        }

        return { reply, chunks_used: chunksForClient, reasoning_steps: reasoningSteps };
    }

    async saveChat(userId, conversationId, question, reply, metadata) {
        // Decide conversation title if new
        const [existingMessages] = await pool.execute(
            'SELECT COUNT(*) as count FROM user_questions WHERE user_id = ? AND conversation_id = ?',
            [userId, conversationId]
        );
        let conversationTitle = null;
        if (existingMessages[0].count === 0) {
            conversationTitle = question.trim().substring(0, 50);
        }
        await pool.execute(
            'INSERT INTO user_questions (user_id, conversation_id, conversation_title, question, bot_reply, is_answered, metadata) VALUES (?, ?, ?, ?, ?, ?, ?)',
            [userId, conversationId, conversationTitle, question, reply, true, JSON.stringify(metadata)]
        );
    }

    async streamChat({ userId, message, model, conversationId }, sendEvent) {
        // Validate Model
        const modelConfig = (model && model.url && model.name)
            ? model
            : { url: 'https://api.openai.com/v1', name: 'gpt-4o-mini' };

        // Step 1: Router
        sendEvent('status', { content: 'ðŸ§­ Äang phÃ¢n tÃ­ch ngá»¯ cáº£nh & cÃ¢u há»i...' });

        // Prepare History & Context
        let history = [];
        let processingMessage = message;
        if (userId && conversationId) {
            history = await this.getChatHistory(userId, conversationId);
            if (history.length > 0) {
                processingMessage = await this.rewriteQuery(message, history, modelConfig);
            }
        }

        const { intent, reasoning } = await classifyIntent(processingMessage, modelConfig);
        sendEvent('status', { content: `ðŸ” Intent detected: ${intent}` });

        let reply = '';
        let reasoningDetail = [`Intent: ${intent}`];
        let chunksUsed = [];
        let webSources = [];
        let sourceType = 'stream';
        let finalConversationId = conversationId;
        const streamStartTime = Date.now();

        try {
            // Case 1: Greeting
            if (intent === INTENTS.GREETING) {
                sendEvent('status', { content: 'ðŸ‘‹ Äang soáº¡n cÃ¢u tráº£ lá»i...' });
                const directReply = await callLLM(modelConfig, [
                    { role: 'system', content: "Báº¡n lÃ  trá»£ lÃ½ AI thÃ¢n thiá»‡n. HÃ£y tráº£ lá»i ngáº¯n gá»n." },
                    ...history.slice(-4),
                    { role: 'user', content: message }
                ]);
                reply = directReply;
                sendEvent('text', { content: reply });
            }
            // Case 2: Live Search
            else if (intent === INTENTS.LIVE_SEARCH) {
                sendEvent('status', { content: 'ðŸŒ Äang tÃ¬m kiáº¿m trÃªn internet...' });
                const searchResult = await performWebSearch(processingMessage);
                const { context: searchContext, sources } = searchResult;
                webSources = sources;
                sourceType = 'web_search';

                sendEvent('status', { content: 'ðŸ“ Äang tá»•ng há»£p thÃ´ng tin...' });
                const systemPrompt = `Báº¡n lÃ  má»™t trá»£ lÃ½ cáº­p nháº­t tin tá»©c thÃ´ng minh. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng dá»±a trÃªn káº¿t quáº£ tÃ¬m kiáº¿m web má»›i nháº¥t Ä‘Æ°á»£c cung cáº¥p.\nThá»i gian hiá»‡n táº¡i: ${new Date().toLocaleString('vi-VN')}\n\nYÃªu cáº§u:\n1. Tráº£ lá»i chÃ­nh xÃ¡c, ngáº¯n gá»n.\n2. DáºªN NGUá»’N (Link URL) á»Ÿ cuá»‘i cÃ¢u tráº£ lá»i dáº¡ng [Title](URL).\n3. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, hÃ£y thÃ nh tháº­t nÃ³i khÃ´ng biáº¿t.\n4. TrÃ¬nh bÃ y Ä‘áº¹p báº±ng Markdown.`;

                const replyRaw = await callLLM(modelConfig, [
                    { role: 'system', content: systemPrompt },
                    ...history.slice(-4),
                    { role: 'user', content: `# CÃ¢u há»i: ${message}\n\n${searchContext}` }
                ], 0.4, 800);
                reply = toAdvancedMarkdown(replyRaw);
                reasoningDetail.push(
                    `Performed Web Search via Tavily AI`,
                    `Synthesized answer from ${webSources.length} web results`
                );
                sendEvent('text', { content: reply });
            }
            // Case 3: Knowledge RAG
            else if (intent === INTENTS.KNOWLEDGE) {
                sendEvent('status', { content: 'ðŸ§  Äang tra cá»©u dá»¯ liá»‡u ná»™i bá»™...' });
                const questionEmbedding = await getEmbedding(processingMessage);
                const rawChunks = await multiStageRetrieval(questionEmbedding, processingMessage, 5);
                chunksUsed = rawChunks.map(c => ({
                    id: c.id,
                    title: c.title,
                    content: c.content,
                    score: c.score,
                    source: c.source_type || 'vector',
                    stage: c.retrieval_stage || 'retrieval'
                }));

                if (rawChunks.length === 0) {
                    // Fallback: Thá»­ tÃ¬m trÃªn Web thay vÃ¬ bá» cuá»™c
                    sendEvent('status', { content: 'ðŸ“­ KhÃ´ng tÃ¬m tháº¥y trong tÃ i liá»‡u, Ä‘ang thá»­ tÃ¬m trÃªn web...' });
                    try {
                        const searchResult = await performWebSearch(processingMessage);
                        const { context: searchContext, sources } = searchResult;
                        if (searchContext && !searchContext.includes('chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh') && !searchContext.includes('gáº·p lá»—i')) {
                            sendEvent('status', { content: 'ðŸ“ Äang tá»•ng há»£p tá»« káº¿t quáº£ web...' });
                            const fallbackPrompt = `Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh. CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng khÃ´ng tÃ¬m tháº¥y trong cÆ¡ sá»Ÿ dá»¯ liá»‡u ná»™i bá»™, nÃªn báº¡n sáº½ tráº£ lá»i dá»±a trÃªn káº¿t quáº£ tÃ¬m kiáº¿m web.\nThá»i gian hiá»‡n táº¡i: ${new Date().toLocaleString('vi-VN')}\n\nYÃªu cáº§u:\n1. Tráº£ lá»i chÃ­nh xÃ¡c, ngáº¯n gá»n.\n2. DáºªN NGUá»’N (Link URL) á»Ÿ cuá»‘i cÃ¢u tráº£ lá»i dáº¡ng [Title](URL).\n3. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, hÃ£y thÃ nh tháº­t nÃ³i khÃ´ng biáº¿t.\n4. TrÃ¬nh bÃ y Ä‘áº¹p báº±ng Markdown.\n5. LÆ°u Ã½: Káº¿t quáº£ nÃ y tá»« internet, KHÃ”NG pháº£i tá»« tÃ i liá»‡u ná»™i bá»™.`;

                            const replyRaw = await callLLM(modelConfig, [
                                { role: 'system', content: fallbackPrompt },
                                ...history.slice(-4),
                                { role: 'user', content: `# CÃ¢u há»i: ${message}\n\n${searchContext}` }
                            ], 0.4, 800);
                            reply = toAdvancedMarkdown(replyRaw);
                            webSources = sources;
                            sourceType = 'kb_fallback_web';
                            reasoningDetail.push(
                                `Retrieval returned 0 relevant chunks from KB`,
                                `Fallback: Performed Web Search via Tavily AI`,
                                `Synthesized answer from ${sources.length} web results`
                            );
                        } else {
                            reply = "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong tÃ i liá»‡u ná»™i bá»™ vÃ  cÅ©ng khÃ´ng thá»ƒ tÃ¬m trÃªn web.";
                        }
                    } catch (fallbackError) {
                        console.warn('âš ï¸ Stream Web Search fallback failed:', fallbackError.message);
                        reply = "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong tÃ i liá»‡u.";
                    }
                } else {
                    sendEvent('status', { content: 'ðŸ’¡ Äang suy luáº­n...' });
                    const fusedContext = fuseContext(rawChunks, [], processingMessage);
                    reply = await callLLM(modelConfig, [
                        { role: 'system', content: "Tráº£ lá»i cÃ¢u há»i dá»±a trÃªn context sau:\n" + fusedContext },
                        ...history.slice(-6),
                        { role: 'user', content: message }
                    ]);
                }
                sendEvent('text', { content: reply });
            }
            // Case 4: Off Topic
            else {
                reply = "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ tráº£ lá»i cÃ¢u há»i nÃ y.";
                sendEvent('text', { content: reply });
            }

            // Save to DB
            if (userId) {
                finalConversationId = await conversationService.getOrCreateConversationId(userId, conversationId);
                const processTime = Date.now() - streamStartTime;
                const isWebSearch = sourceType === 'web_search' || sourceType === 'kb_fallback_web';
                const metadata = {
                    processing_time: processTime,
                    model: modelConfig.name,
                    total_chunks: chunksUsed.length,
                    intent: intent,
                    source: sourceType
                };
                reasoningDetail.push(`Processing time: ${processTime}ms`);
                await this.saveChat(userId, finalConversationId, message, reply, metadata);
                await usageService.trackUsage(userId, isWebSearch ? 'web_search' : 'stream_chat', { tokens: reply.length / 4 });
            }

            // Finalize - Gap 4: Include web_sources + source_type
            sendEvent('done', {
                reply,
                reasoning_steps: reasoningDetail,
                chunks_used: chunksUsed,
                conversationId: finalConversationId,
                source_type: sourceType,
                web_sources: webSources
            });

        } catch (error) {
            console.error('Stream processing error:', error);
            throw error;
        }
    }

    async getHistory(userId) {
        const [rows] = await pool.execute(
            `SELECT id, question, bot_reply, is_answered, created_at 
       FROM user_questions 
       WHERE user_id = ? 
       ORDER BY created_at DESC 
       LIMIT 50`,
            [userId]
        );
        return rows;
    }

    async getAdvancedRAGStats() {
        const [stats] = await pool.execute(`
      SELECT 
        COUNT(*) as total_questions,
        AVG(CAST(metadata->>'total_chunks' AS NUMERIC)) as avg_chunks,
        AVG(CAST(metadata->>'processing_time' AS NUMERIC)) as avg_processing_time,
        COUNT(CASE WHEN CAST(metadata->>'reasoning_chains' AS NUMERIC) > 0 THEN 1 END) as complex_questions
      FROM user_questions 
      WHERE metadata IS NOT NULL
    `);
        return stats[0];
    }
}

export default new ChatService();
