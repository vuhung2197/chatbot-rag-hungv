import fs from 'fs';
import speakingRepository from '../repositories/speaking.repository.js';
import speakingAiService from './speakingAI.service.js';
import azureSpeechService from './azureSpeech.service.js';
import analyticsService from '../../analytics/services/analytics.service.js';
import OpenAI from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export const speakingService = {

    // ==================== TOPICS ==================== //

    async getTopics({ type, level, page = 1, limit = 10 }) {
        const offset = (page - 1) * limit;
        const { topics, total } = await speakingRepository.getTopics({ type, level, limit, offset });
        return {
            topics,
            pagination: { page, limit, total, totalPages: Math.ceil(total / limit) }
        };
    },

    async getTopicById(id) {
        const topic = await speakingRepository.getTopicById(id);
        if (!topic) throw new Error('Topic not found');
        return topic;
    },

    // Quáº£n lÃ½ viá»‡c táº¡o Audio TTS tá»± Ä‘á»™ng cho Topic (DÃ¹ng cho máº«u Ä‘á»c Shadowing vÃ  Ä‘á»c cÃ¢u há»i)
    async ensureTopicAudio(id) {
        const topic = await this.getTopicById(id);
        if (topic.audio_url) return topic.audio_url; // ÄÃ£ cÃ³

        // ChÆ°a cÃ³ -> Generate báº±ng AI TTS alloy
        console.log(`ðŸŽ™ï¸ Sinh TTS audio cho topic ID: ${id}`);
        const mp3 = await openai.audio.speech.create({
            model: 'tts-1', voice: 'alloy', input: topic.prompt_text
        });
        const buffer = Buffer.from(await mp3.arrayBuffer());

        // Thay vÃ¬ lÆ°u cloud, MVP nÃ y ta return stream buffer tháº³ng cho /audio/:id (giá»‘ng Listening)
        // Ä‘á»ƒ tiáº¿t kiá»‡m tiá»n/cÃ´ng setup S3, do Ä‘Ã³ audio_url chá»‰ lÃ  cá» Ä‘Ã¡nh dáº¥u náº¿u file tÄ©nh.
        // Táº¡m thá»i náº¿u audio_url == null -> frontend gá»i API /api/speaking/topics/:id/audio
        return buffer;
    },

    // ==================== PRONUNCIATION (IPA) ==================== //

    async getIpaPhonemes() {
        return await speakingRepository.getIpaPhonemes();
    },

    // ==================== SUBMISSIONS & GRADING ==================== //

    async submitAudio(userId, { topicId, audioFilePath }) {
        let submissionId;
        try {
            const topic = await this.getTopicById(topicId);
            if (!topic) throw new Error('Topic khÃ´ng tá»“n táº¡i');

            // 1. LÆ°u Record khá»Ÿi Ä‘áº§u (Ä‘á»ƒ user cÃ³ thá»ƒ xem láº¡i lá»‹ch sá»­ sau nÃ y náº¿u MVP nÃ¢ng cáº¥p lÆ°u db audio)
            const submission = await speakingRepository.createSubmission({ userId, topicId, audioUrl: null });
            submissionId = submission.id;

            // 2. Transcribe Audio (Whisper) - Always done to ensure we have a transcript
            let transcript = '';
            try {
                transcript = await speakingAiService.transcribeAudio(audioFilePath);
            } catch (e) {
                console.warn('Whisper transcription failed (will be ignored if Azure works):', e);
            }

            // 3. Grade the transcript based on Topic type
            let evaluation;
            if (topic.type === 'shadowing' || topic.type === 'pronunciation') {
                // Sá»­ dá»¥ng Azure Cognitive Services cho pháº§n Ä‘Ã¡nh giÃ¡ Ã¢m vá»‹ cá»±c chuáº©n
                try {
                    console.log(`ðŸŽ™ï¸ Cháº¥m Ä‘iá»ƒm báº±ng Azure cho topic: ${topic.type}`);
                    evaluation = await azureSpeechService.evaluatePronunciation(audioFilePath, topic.prompt_text);
                } catch (e) {
                    console.warn(`Azure failed, fallback to Whisper AI: ${e.message}`);
                    evaluation = topic.type === 'shadowing' ?
                        await speakingAiService.gradeShadowing(topic.level, topic.prompt_text, transcript) :
                        await speakingAiService.gradePronunciation(topic.level, topic.prompt_text, transcript);

                    if (!transcript) transcript = evaluation.transcript || '';
                }
                if (!transcript && evaluation.transcript) {
                    transcript = evaluation.transcript;
                }
            } else if (topic.type === 'reflex') {
                evaluation = await speakingAiService.gradeReflex(topic.level, topic.prompt_text, transcript);
            } else { // 'topic'
                evaluation = await speakingAiService.gradeTopic(topic.level, topic.prompt_text, transcript);
            }

            // 4. Update Submission result
            const newWords = evaluation.advanced_vocabulary || []; // Topic
            const scoreTotal = evaluation.score || 0;
            const updated = await speakingRepository.updateSubmissionAfterAI(submissionId, {
                transcript, scoreTotal, feedback: evaluation, newWords, status: 'completed'
            });

            // XÃ³a file Ã¢m thanh táº¡m ngay láº­p tá»©c
            if (fs.existsSync(audioFilePath)) fs.unlinkSync(audioFilePath);

            // 5. ThÃªm tá»« má»›i / lá»—i phÃ¡t Ã¢m vÃ o Sá»• tay Knowledge Hub
            const pronunciationItems = topic.type === 'shadowing' && evaluation.mistakes ? evaluation.mistakes : [];
            const grammarItems = (topic.type === 'topic' || topic.type === 'reflex') && evaluation.errors ? evaluation.errors.map(e => ({
                word: e.correction,
                definition: e.explanation,
                grammar_error: e.mistake,
                grammar_correction: e.correction,
                level: topic.level
            })) : [];

            // Auto-log mistakes to analytics
            pronunciationItems.forEach(item => {
                analyticsService.logMistake({
                    userId,
                    sourceModule: 'speaking',
                    errorCategory: 'pronunciation',
                    errorDetail: item.expected || 'phoneme_error',
                    contextText: item.heard || '',
                    sessionId: submissionId
                }).catch(e => console.error('Silent fail on log mistake:', e));
            });

            grammarItems.forEach(item => {
                analyticsService.logMistake({
                    userId,
                    sourceModule: 'speaking',
                    errorCategory: 'grammar',
                    errorDetail: 'grammar_error',
                    contextText: item.grammar_error || '',
                    sessionId: submissionId
                }).catch(e => console.error('Silent fail on log mistake:', e));
            });


            if (newWords.length > 0 || pronunciationItems.length > 0 || grammarItems.length > 0) {
                // Grammar items trong Speaking Ä‘Æ°á»£c append vÃ o pronunciationItems param Ä‘á»ƒ process chung á»Ÿ hÃ m tiáº¿p theo 
                // (hoáº·c náº¿u repository update láº¡i addKnowledgeBatch há»— trá»£ grammar)

                // Note: Ta gá»™p pronunciationItems vÃ  grammarItems vÃ o máº£ng chung Ä‘á»ƒ hÃ m addKnowledgeBatch á»Ÿspeaking repository xá»­ lÃ½
                // Äá»ƒ Ä‘Æ¡n giáº£n MVP, speakingRepository.addKnowledgeBatch nháº­n tham sá»‘ (userId, words, pronunciationItems, submissionId)
                // Ta map grammarItems thÃ nh Ä‘á»‹nh dáº¡ng pronunciation Ä‘á»ƒ vÃ o chung 1 báº£ng táº¡m. Hoáº·c update luÃ´n speakingRepository há»— trá»£ máº£ng thá»© 3 cho Grammar.
                // Cho gá»n thÃ¬ ta sá»­a hÃ m gá»i:
                await speakingRepository.addKnowledgeBatch(userId, newWords, [...pronunciationItems, ...grammarItems.map(g => ({
                    expected: g.grammar_correction,
                    tip: g.definition,
                    heard: g.grammar_error
                }))], submissionId)
                    .catch(e => console.error('Knowledge batch error (speaking):', e));
            }

            return { ...updated, topic_type: topic.type };

        } catch (error) {
            console.error('Speaking submit failed:', error);
            if (submissionId) {
                await speakingRepository.updateSubmissionAfterAI(submissionId, {
                    transcript: '', scoreTotal: 0, feedback: { error: error.message }, newWords: [], status: 'error'
                }).catch(() => { });
            }
            if (fs.existsSync(audioFilePath)) fs.unlinkSync(audioFilePath);
            throw error;
        }
    }
};

export default speakingService;
